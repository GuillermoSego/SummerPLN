{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las variables de entorno del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave API de OpenAI desde la variable de entorno\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Crear un cliente de OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Definir el prompt y los parámetros de la solicitud\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "# Imprimir la respuesta generada\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial (IA) ha revolucionado la medicina en los últimos años, ofreciendo un sinfín de posibilidades para mejorar la precisión, eficiencia y calidad de la atención médica. Con la capacidad de analizar grandes cantidades de datos de forma rápida y precisa, la IA se ha convertido en una herramienta invaluable para los profesionales de la salud en la toma de decisiones clínicas y diagnósticos.\n",
      "\n",
      "Una de las\n"
     ]
    }
   ],
   "source": [
    "# Definir el prompt y los parámetros de la solicitud\n",
    "complex_prompt = \"\"\"\n",
    "Escribe un ensayo detallado sobre la importancia de la inteligencia artificial en la medicina. Incluye las áreas clave donde se está aplicando, los beneficios que aporta, los desafíos que enfrenta y el futuro de esta tecnología en el campo médico.\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": complex_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=100,  # Aumenta el límite de tokens para respuestas más largas\n",
    "    temperature=0.7,  # Ajusta la creatividad\n",
    ")\n",
    "\n",
    "# Imprimir la respuesta generada\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dd22ff1b9a4b5ea2cef83cb5d3536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa37995229c14a06badc44429444c103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b9cb33b81449468907c84b0288acdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d2a778277341ceb72650636b3a38a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32e9bb2e807462ba5bdd449e84358b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e959aca8e643488bca09d6fcc9e7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 'Hello, how are you?'</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Cargar modelo y tokenizer desde Hugging Face\n",
    "model_name = \"t5-small\" \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Definir el texto de entrada\n",
    "input_text = \"Translate English to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Tokenizar la entrada y generar la salida\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decodificar y mostrar la salida\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Generate a simple text of presentation</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Generate a simple text of presentation\"\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage After Loading Model: 48.90624 MB\n",
      "Decoded Output: <pad> 'Hello, how are you?'</s>\n",
      "Memory Usage After Generation: 366.493696 MB\n",
      "Additional Memory Usage During Generation: 317.587456 MB\n"
     ]
    }
   ],
   "source": [
    "# Función para medir el uso de memoria\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1e6  # Convertir a MB\n",
    "\n",
    "\n",
    "# Medir uso de memoria después de cargar el modelo\n",
    "memory_usage_after_loading = get_memory_usage()\n",
    "print(f\"Memory Usage After Loading Model: {memory_usage_after_loading} MB\")\n",
    "\n",
    "# Tokenizar entrada de ejemplo\n",
    "input_text = \"Translate English to French: 'Hello, how are you?'\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generar salida\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decodificar salida\n",
    "decoded_output = tokenizer.decode(outputs[0])\n",
    "print(f\"Decoded Output: {decoded_output}\")\n",
    "\n",
    "# Medir uso de memoria después de la generación\n",
    "memory_usage_after_generation = get_memory_usage()\n",
    "print(f\"Memory Usage After Generation: {memory_usage_after_generation} MB\")\n",
    "\n",
    "# Uso de memoria adicional durante la generación\n",
    "additional_memory_usage = memory_usage_after_generation - memory_usage_after_loading\n",
    "print(f\"Additional Memory Usage During Generation: {additional_memory_usage} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
