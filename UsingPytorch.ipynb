{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Pytorch\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "El auge de las redes neuronales se da gracias a la posibilidad de paralelización de operaciones en una computadora. Una CPU puede realizar tareas secuenciales en sus múltiples núcleos, sin embargo una GPU se especializa en ejecutar operaciones en paralelo en sus múltiples núcleos. Esto permite poder realizar renderizado mas rápido, o mostrar video de mejor calidad, ya que son tareas que se pueden paralelizar. En el contexto de la inteligencia artificial, el entrenamiento de un modelo también es paralelizable. La librería Pytorch proporciona una clase central, los **tensores** de pytorch, que son similares a los arrays de numpy salvo porque los tensores pueden trabajar en GPU.\n",
    "\n",
    "Para poder trabajar con GPU, Pytorch tiene soporte en distintas plataformas que poseen GPU. La plataforma mas utilizada y mejor integrada con pytorch es CUDA de NVIDIA. Sin embargo existen diferentes maneras de aprovechar la GPU en pytorch:\n",
    "\n",
    "* **CUDA**: NVDIA es una empresa dedicada al diseño y fabricación de procesadores GPU. NVDIA proporciona una API (Interfaz de Programación de Aplicaciones) llamada **CUDA** que permite la creación de programas paralelos masivos que utilizan GPU. \n",
    "* **ROCm**:  PyTorch también tiene soporte para GPUs de AMD mediante ROCm (Radeon Open Compute). Aunque ROCm no está tan ampliamente soportado como CUDA, es una opción viable para aquellos con hardware AMD compatible.\n",
    "* **MPS**: Para dispositivos Apple con GPUs integradas (como los chips M1 y M2), PyTorch ha introducido soporte experimental para MPS. Esto permite utilizar la GPU a través de la API Metal de Apple.\n",
    "* **CPU**: Aunque no es tan rápido como una GPU, PyTorch puede ejecutar modelos en paralelo utilizando múltiples núcleos de CPU. Esto es especialmente útil en entornos donde no hay GPU disponible.\n",
    "* **TPU**: Para tareas intensivas en la nube, PyTorch también puede ser utilizado con TPUs a través de PyTorch/XLA. TPUs son hardware especializado diseñado por Google para acelerar la carga de trabajo de aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores \n",
    "\n",
    "La clase central de PyTorch es el tensor. Todo esta basado en tensores. Uno de los atributos de este tipo de clase son las dimensiones (1d, 2d, 3d, ...).\n",
    "\n",
    "Podemos definir un tensor como sigue:\n",
    "\n",
    "`torch.tensor(data, dtype=None, device=None, requires_grad=False)`\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* `data` (`array_like`): los datos para el tensor\n",
    "* `dtype` (`torch.dtype`): el tipo de dato del tensor. Cuando es `None` infiere el tipo de dato. Otros tipos pueden ser: `torch.int`, `torch.float`, `torch.double`, etc...\n",
    "* `device` (`torch.device`): el dispositivo donde el tensor se va a almacenar (CPU o GPU).\n",
    "* `requires_grad` (`bool`): si el autogradiente debe acumular operaciones con el tensor. Si va requerir cálculos de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir de muchas maneras un tensor de pytorch, asi como un array de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\INICIALIZADO\n",
      "Tensor: tensor(1.) \n",
      "shape: torch.Size([]) \n",
      "dtype: torch.float32 \n",
      "device: cpu \n",
      "requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Tensor Escalar\n",
    "\n",
    "print('\\INICIALIZADO')\n",
    "x = torch.tensor(1.0) # Definición tensor escalar\n",
    "print('Tensor:', x, '\\nshape:', x.shape, '\\ndtype:', x.dtype, '\\ndevice:', x.device, '\\nrequires_grad:', x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\SIN INICIALIZAR\n",
      "Tensor: tensor([0., 0., 0.]) \n",
      "shape: torch.Size([3]) \n",
      "dtype: torch.float32 \n",
      "device: cpu \n",
      "requires_grad: False\n",
      "\n",
      "A PARTIR DE UNA LISTA\n",
      "Tensor: tensor([1, 2, 3]) \n",
      "shape: torch.Size([3]) \n",
      "dtype: torch.int64 \n",
      "device: cpu \n",
      "requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Lista a Tensor\n",
    "print('\\SIN INICIALIZAR')\n",
    "x = torch.empty(3) # vector, 1D\n",
    "print('Tensor:', x, '\\nshape:', x.shape, '\\ndtype:', x.dtype, '\\ndevice:', x.device, '\\nrequires_grad:', x.requires_grad)\n",
    "\n",
    "print('\\nA PARTIR DE UNA LISTA')\n",
    "x = torch.tensor([1,2,3])\n",
    "print('Tensor:', x, '\\nshape:', x.shape, '\\ndtype:', x.dtype, '\\ndevice:', x.device, '\\nrequires_grad:', x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIN INICIALIZAR\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "INICIALIZADO\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Matriz\n",
    "print('SIN INICIALIZAR')\n",
    "x = torch.empty(2,3) # Dimensiones de la matriz. Matriz 2D de 2x3\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print('\\nINICIALIZADO')\n",
    "x = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor, 3 dimensiones\n",
    "x = torch.empty(2,4,3) # El tensor tiene 2 matrices de 4x3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]], dtype=torch.float64)\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Numpy array a tensor\n",
    "x = torch.tensor(np.zeros((2,2,2)))\n",
    "print(x)\n",
    "\n",
    "a = np.zeros((2,2,2))\n",
    "x = torch.from_numpy(a)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aleatorio\n",
      "tensor([[0.9411, 0.6003, 0.9732],\n",
      "        [0.0778, 0.7290, 0.5075],\n",
      "        [0.8284, 0.9292, 0.9620],\n",
      "        [0.8658, 0.1751, 0.3327],\n",
      "        [0.8522, 0.6978, 0.0723]])\n",
      "Tensor con ceros\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Tensor con unos\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Otras inicializaciones\n",
    "print(\"Aleatorio\")\n",
    "x = torch.rand(5, 3) # torch.rand(size)\n",
    "print(x)\n",
    "\n",
    "print(\"Tensor con ceros\")\n",
    "x = torch.zeros(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"Tensor con unos\")\n",
    "x = torch.ones(2, 3, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de tensores\n",
    "\n",
    "Los tensores al igual que los arreglos de numpy pueden realizar múltiples operaciones entre sí.\n",
    "\n",
    "Para poder acceder a los elementos de un tensor, tenemos el mismo código que un array de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tensor completo: \n",
      " tensor([[0.7717, 0.7330],\n",
      "        [0.7602, 0.3112],\n",
      "        [0.4237, 0.8438]], dtype=torch.float64)\n",
      "Primera columna: \n",
      " tensor([0.7717, 0.7602, 0.4237], dtype=torch.float64)\n",
      "Primer renglon:\n",
      " tensor([0.7717, 0.7330], dtype=torch.float64)\n",
      "0.311165177604677\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "x = torch.tensor(np.random.rand(3, 2))\n",
    "print(\"El tensor completo: \\n\", x)\n",
    "\n",
    "print(\"Primera columna: \\n\", x[:,0])\n",
    "print(\"Primer renglon:\\n\", x[0,:])\n",
    "\n",
    "# Obtener el valor si solo hay un elemento en el tensor\n",
    "print(x[1,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de instancia **view()** en PyTorch se utiliza para cambiar la forma (reshape) de un tensor sin cambiar sus datos subyacentes. Es decir, podemos modificar la forma en la que se estructuran los datos de un tensor. Esto es súper útil en redes neuronales para preparar los datos para los modelos de aprendizaje profundo que requieren entradas específicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor original\n",
      " tensor([[0.9597, 0.9857, 0.6986, 0.7822],\n",
      "        [0.5193, 0.4187, 0.1110, 0.2459],\n",
      "        [0.3241, 0.4581, 0.6066, 0.9271],\n",
      "        [0.1685, 0.2506, 0.8812, 0.5939]])\n",
      "tensor([0.9597, 0.9857, 0.6986, 0.7822, 0.5193, 0.4187, 0.1110, 0.2459, 0.3241,\n",
      "        0.4581, 0.6066, 0.9271, 0.1685, 0.2506, 0.8812, 0.5939])\n",
      "tensor([[0.9597, 0.9857, 0.6986, 0.7822, 0.5193, 0.4187, 0.1110, 0.2459],\n",
      "        [0.3241, 0.4581, 0.6066, 0.9271, 0.1685, 0.2506, 0.8812, 0.5939]])\n",
      "tensor([[[0.9597, 0.9857, 0.6986, 0.7822],\n",
      "         [0.5193, 0.4187, 0.1110, 0.2459]],\n",
      "\n",
      "        [[0.3241, 0.4581, 0.6066, 0.9271],\n",
      "         [0.1685, 0.2506, 0.8812, 0.5939]]])\n",
      "Shapes\n",
      "\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Definimos un tensor random\n",
    "x = torch.rand(4, 4)\n",
    "y = x.view(16) # Llamamos al método view\n",
    "z = x.view(-1, 8)  # Con -1, pytorch determina la dimensión con base en el tamaño, automáticamente.\n",
    "w = x.view(2,2,-1)\n",
    "\n",
    "print(\"Tensor original\\n\", x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(w)\n",
    "\n",
    "print(\"Shapes\\n\")\n",
    "print(x.size(), y.size(), z.size(), w.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# De torch.tensor a numpy.array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy() # <----\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU o GPU\n",
    "Por default, todos los tensores son creados en CPU. Si está disponible, podemos moverlos a GPU. En macbook lamentablemente no podemos utilizar la API CUDA de NVDIA, sin embargo podemos probar la alternativa de Apple MPS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dispositivo es: mps\n",
      "tensor([[1.9597, 1.9857, 1.6986, 1.7822],\n",
      "        [1.5193, 1.4187, 1.1110, 1.2459],\n",
      "        [1.3241, 1.4581, 1.6066, 1.9271],\n",
      "        [1.1685, 1.2506, 1.8812, 1.5939]], device='mps:0')\n",
      "[[1.9596542 1.9857001 1.6985526 1.7821869]\n",
      " [1.5193303 1.4186745 1.1110432 1.2459285]\n",
      " [1.3241477 1.4581165 1.6066058 1.9270604]\n",
      " [1.1684996 1.2506006 1.8811674 1.5939313]]\n"
     ]
    }
   ],
   "source": [
    "# Verificar si MPS está disponible\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"El dispositivo es:\", device)\n",
    "y = torch.ones_like(x, device=device)  # Crear tensor en GPU\n",
    "x = x.to(device) # Enviar tensor a GPU\n",
    "\n",
    "# Realizar operaciones en GPU\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# z = z.numpy()    # error porque numpy no soporta tensores en GPU\n",
    "z = z.to(\"cpu\")       # mover a cpu\n",
    "z = z.numpy()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ademas determinar cuantos recursos alojan nuestros tensores. (Solo correr en CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor que se guarda en RAM\n",
    "x = torch.tensor(np.zeros((10000, 10000)))\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device,\n",
    "      '\\nnumber of gpus:', torch.cuda.device_count(),\n",
    "      '\\nname of gpu:', torch.cuda.get_device_name(0))\n",
    "print('consumo de VRAM:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones con tensores\n",
    "\n",
    "Podemos realizar operaciones entre tensores asi como si tuviéramos arrays de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1,2,3],\n",
    "                  [4,5,6],\n",
    "                  [7,8,9]], dtype = torch.float64)\n",
    "\n",
    "print(A)\n",
    "\n",
    "B = torch.tensor([[1,0,0],\n",
    "                  [0,1,0],\n",
    "                  [0,0,1]], dtype = torch.float64)\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+B = tensor([[ 2.,  2.,  3.],\n",
      "        [ 4.,  6.,  6.],\n",
      "        [ 7.,  8., 10.]], dtype=torch.float64) \n",
      "\n",
      "A*B = tensor([[1., 0., 0.],\n",
      "        [0., 5., 0.],\n",
      "        [0., 0., 9.]], dtype=torch.float64) \n",
      "\n",
      "AB = tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64) \n",
      "\n",
      "A^2 = tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]], dtype=torch.float64) \n",
      "\n",
      "A^T = tensor([[1., 4., 7.],\n",
      "        [2., 5., 8.],\n",
      "        [3., 6., 9.]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A+B =', A+B, '\\n')\n",
    "print('A*B =', A*B, '\\n')\n",
    "print('AB =', torch.matmul(A,B), '\\n')\n",
    "print('A^2 =', A**2, '\\n')\n",
    "print('A^T =', torch.transpose(A, 0, 1), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autogradiente\n",
    "\n",
    "Cuando optimizamos una red neuronal es necesario utilizar el algoritmo de **backpropagation**. Backpropagation, funciona para poder ejecutar descenso de gradiente en la red neuronal, configurando los pesos para ser óptimos. Optimizar una red neuronal consiste entonces, en elegir una función de pérdida que es la que va minimizar y un algoritmo como SGD (Sthocastic Gradient Des.). Este algoritmo necesita calcular la derivada parcial de la función de costo con respecto a los pesos, para poder ejecutar cada iteración. Es por esta razón que necesitamos *acumular* el gradiente. Al acumular el gradiente es muy sencillo calcular esta derivada y por ende la siguiente iteración del algoritmo. \n",
    "\n",
    "Con el parámetro ***requires_grad*** se le señala a pytorch que se requerirá calcular gradientes para ese tensor en particular. Es decir, se señala la variable que se quiere optimizar en algún modelo. Provee de diferenciación automática para todas las operaciones en el tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3], requires_grad=True)  # requires_grad = True -> registra todas las operaciones en el tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la función\n",
    "\n",
    "$ y = f(a,x,b) = ax^2 + b $,\n",
    "\n",
    "sus derivadas parciales y gradiente son\n",
    "\n",
    "$ \\dfrac{dy}{da} = x^2, \\quad \\dfrac{dy}{dx} = 2ax, \\quad \\dfrac{dy}{db} = 1, \\quad \\nabla y(a,x,b) = (x^2,2ax,1)  $\n",
    "\n",
    "así que evaluando el gradiente en el punto (2,1,1) se tiene que\n",
    "\n",
    "$ \\nabla y(2,1,1) = (1,4,1)  $\n",
    "\n",
    "**Vamos a calcular este gradiente de forma numérica utilizando el autograd de PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "None\n",
      "tensor(3., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x145107e50>\n"
     ]
    }
   ],
   "source": [
    "# Declaramos los tensores. En los valores es el punto en el que queremos calcular el gradiente.\n",
    "a = torch.tensor(2, dtype = torch.float64, requires_grad = False)\n",
    "x = torch.tensor(1, dtype = torch.float64, requires_grad = True)\n",
    "b = torch.tensor(1, dtype = torch.float64, requires_grad = False)\n",
    "\n",
    "# Tensor y\n",
    "# La variable y fue creada como resultado de una operación con x, por lo que y contiene un atributo grad_fn\n",
    "y = a * x**2 + b\n",
    "\n",
    "\n",
    "print(x)\n",
    "print(x.grad_fn) # creada por el usuario -> grad_fn = None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En PyTorch, el atributo `grad_fn` de un tensor es un objeto que representa la función que creó el tensor. Este atributo es una parte fundamental del sistema de autograd de PyTorch, que permite el cálculo automático de gradientes para la optimización de redes neuronales.\n",
    "\n",
    "### Detalles del Atributo `grad_fn`\n",
    "\n",
    "1. **Tensores Creados Manualmente (Por el Usuario)**:\n",
    "   - Cuando creas un tensor manualmente, como en `x = torch.tensor(1, dtype=torch.float64, requires_grad=True)`, no hay una función que haya creado este tensor directamente; por lo tanto, su `grad_fn` es `None`.\n",
    "   - Esto indica que el tensor no es el resultado de una operación diferenciable y es, en cambio, un tensor de entrada en el gráfico computacional.\n",
    "\n",
    "2. **Tensores Resultantes de Operaciones**:\n",
    "   - Cuando un tensor es el resultado de una operación entre otros tensores, su `grad_fn` no será `None`. En cambio, será un objeto que indica qué operación creó el tensor.\n",
    "   - Este objeto permite a PyTorch construir un gráfico computacional dinámico y rastrear cómo los tensores están relacionados a través de las operaciones. Esto es crucial para calcular gradientes mediante el algoritmo de backpropagation.\n",
    "\n",
    "\n",
    "### Explicación del Código\n",
    "\n",
    "1. **Tensor `x`**:\n",
    "   - `x` es un tensor creado manualmente con `requires_grad=True`.\n",
    "   - `print(x.grad_fn)` muestra `None` porque `x` no es el resultado de una operación; es un tensor de entrada.\n",
    "\n",
    "2. **Tensor `y`**:\n",
    "   - `y` se calcula como `a * x**2 + b`.\n",
    "   - `y` es el resultado de una serie de operaciones: primero, `x**2`, luego `a * (x**2)`, y finalmente `a * (x**2) + b`.\n",
    "   - `print(y.grad_fn)` muestra algo como `<AddBackward0>` indicando que `y` fue creado por una operación de suma (Add) en PyTorch.\n",
    "\n",
    "### ¿Cómo Funciona `grad_fn`?\n",
    "\n",
    "El atributo `grad_fn` permite a PyTorch construir un gráfico de cómputo dinámico. Cuando realizas operaciones en tensores con `requires_grad=True`, PyTorch guarda las operaciones en un gráfico. Este gráfico se utiliza para calcular los gradientes durante la retropropagación (backpropagation).\n",
    "\n",
    "Por ejemplo, en el cálculo de `y = a * x**2 + b`:\n",
    "- `x**2` tiene un `grad_fn` que indica una operación de potencia.\n",
    "- `a * (x**2)` tiene un `grad_fn` que indica una operación de multiplicación.\n",
    "- `a * (x**2) + b` tiene un `grad_fn` que indica una operación de suma.\n",
    "\n",
    "Durante la retropropagación, PyTorch utiliza estos objetos `grad_fn` para aplicar la regla de la cadena y calcular los gradientes de los tensores con respecto a sus entradas.\n",
    "\n",
    "### Visualización del Gráfico Computacional\n",
    "\n",
    "Para obtener una mejor idea de cómo PyTorch construye y utiliza este gráfico, se puede usar la función `torchviz` para visualizar el gráfico computacional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (20240428.1522)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"108pt\" height=\"337pt\"\n",
       " viewBox=\"0.00 0.00 108.00 336.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-332.5 104,-332.5 104,4 -4,4\"/>\n",
       "<!-- 5172500272 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5172500272</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"77,-32.75 23,-32.75 23,0 77,0 77,-32.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5456374240 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5456374240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-89.5 6,-89.5 6,-68.75 94,-68.75 94,-89.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 5456374240&#45;&gt;5172500272 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5456374240&#45;&gt;5172500272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-68.36C50,-61.89 50,-53.05 50,-44.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-44.55 50,-34.55 46.5,-44.55 53.5,-44.55\"/>\n",
       "</g>\n",
       "<!-- 5456375680 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5456375680</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-146.25 6,-146.25 6,-125.5 94,-125.5 94,-146.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 5456375680&#45;&gt;5456374240 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5456375680&#45;&gt;5456374240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-125.09C50,-118.47 50,-109.47 50,-101.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-101.34 50,-91.34 46.5,-101.34 53.5,-101.34\"/>\n",
       "</g>\n",
       "<!-- 5456375584 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5456375584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-203 6,-203 6,-182.25 94,-182.25 94,-203\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 5456375584&#45;&gt;5456375680 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5456375584&#45;&gt;5456375680</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-181.84C50,-175.22 50,-166.22 50,-158.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-158.09 50,-148.09 46.5,-158.09 53.5,-158.09\"/>\n",
       "</g>\n",
       "<!-- 5456376304 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5456376304</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-259.75 0,-259.75 0,-239 100,-239 100,-259.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-246.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5456376304&#45;&gt;5456375584 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5456376304&#45;&gt;5456375584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-238.59C50,-231.97 50,-222.97 50,-214.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-214.84 50,-204.84 46.5,-214.84 53.5,-214.84\"/>\n",
       "</g>\n",
       "<!-- 5172498640 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5172498640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-328.5 23,-328.5 23,-295.75 77,-295.75 77,-328.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-303\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5172498640&#45;&gt;5456376304 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5172498640&#45;&gt;5456376304</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-295.48C50,-288.1 50,-279.18 50,-271.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-271.41 50,-261.41 46.5,-271.41 53.5,-271.41\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1455a2a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos los gradientes con backpropagation:\n",
    "\n",
    "*   Al terminar todas las operaciones que se le aplicarán al tensor, se puede llamar al método .backward(), que calculará todos los gradientes automáticamente.\n",
    "*   La derivada parcial de la función con respecto al tensor se acumulará en el atributo .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx =  tensor(4., dtype=torch.float64)\n",
      "dy/da =  None\n",
      "dy/db =  None\n"
     ]
    }
   ],
   "source": [
    "# Calculamos los gradientes\n",
    "y.backward()\n",
    "print('dy/dx = ', x.grad)\n",
    "\n",
    "# Estas derivadas no serán calculadas por que el tensor no estaba acumulando\n",
    "print('dy/da = ', a.grad)\n",
    "print('dy/db = ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización durante el entrenamiento de algún modelo\n",
    "\n",
    "Durante el ciclo de entrenamiento, se realizan operaciones con los pesos del modelo y después se requiere **actualizar** los pesos calculados (optimización). Esta operación de actualización de pesos **no debe acumularse** en el gradiente. Para solucionarlo, existen varias alternativas:\n",
    "\n",
    "*  x.requires_grad_(False) $ \\quad$  ----> cambia la bandera existente 'in-place'\n",
    "*  x.detach()             $ \\quad$ ----> obtener un nuevo tensor con el mismo contenido pero sin cálculos de gradiente.\n",
    "*  with torch.no_grad():   $ \\quad$----> envolver instrucciones en 'with torch.no_grad():'\n",
    "\n",
    "\n",
    "Además, .backward() acumula el gradiente para el tensor en el atributo .grad, por lo que se debe ser cuidadoso durante la optimización.\n",
    "\n",
    "Se utiliza\n",
    "> .zero_()\n",
    "\n",
    "para **vaciar la acumulación del gradiente** antes de un nuevo paso de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights - epoch 0: tensor([6., 6., 6., 6.])\n",
      "weights - epoch 1: tensor([2.4000, 2.4000, 2.4000, 2.4000])\n",
      "weights - epoch 2: tensor([0.9600, 0.9600, 0.9600, 0.9600])\n",
      "tensor([0.0640, 0.0640, 0.0640, 0.0640], requires_grad=True)\n",
      "tensor(0.3072, grad_fn=<SumBackward0>)\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de modelo simple\n",
    "weights = torch.ones(4, requires_grad=True) # Pesos que requieren acumulación de gradiente\n",
    "lrRate = 0.1\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (3*weights**2).sum() # Operaciones\n",
    "    \n",
    "    # Acumulamos el gradiente\n",
    "    model_output.backward()\n",
    "\n",
    "    # Imprimimos el gradiente\n",
    "    print(f\"weights - epoch {epoch}: {weights.grad}\")\n",
    "\n",
    "    # Optimizar modelo, actualizar pesos.\n",
    "    # (En la práctica, estos pasos se suelen hacer automáticamente por medio de implementaciones en la librería)\n",
    "    with torch.no_grad(): # No acumula gradiente\n",
    "        weights -= lrRate * weights.grad\n",
    "\n",
    "    # Vaciar gradiente\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra reservada **with** permite utilizar el context manager de la liberia PyTorch para evitar la acumulación de gradientes en tensores específicos. Un context manager es una construcción en Python que asegura que ciertos recursos se inicialicen y limpien adecuadamente. Cuando usas la instrucción with, el context manager se encarga de ejecutar el código necesario antes y después del bloque with. El context manager hace básicamente lo siguiente:\n",
    "\n",
    "1.\tMétodo \\_\\_enter\\_\\_:\n",
    "\t•\tGuarda el estado actual de grad_enabled.\n",
    "\t•\tDesactiva el cálculo de gradientes usando torch.set_grad_enabled(False).\n",
    "2.\tMétodo \\_\\_exit\\_\\_:\n",
    "\t•\tRestaura el estado de grad_enabled al valor que tenía antes de entrar en el contexto.\n",
    "\n",
    "Los algoritmos de optimización implementados en la librería (ejemplo: torch.optim.SGD) incluyen el método zero_grad() para reiniciar la acumulación de gradientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.grad, antes de reiniciar acumulacion de grad:  tensor([0.3840, 0.3840, 0.3840, 0.3840])\n",
      "\n",
      "weights:  tensor([0.0256, 0.0256, 0.0256, 0.0256], requires_grad=True)\n",
      "\n",
      "model_output:  tensor(0.0492, grad_fn=<SumBackward0>)\n",
      "\n",
      "weights.grad, después de reiniciar acumulacion de grad:  None\n"
     ]
    }
   ],
   "source": [
    "# Optimizador SGD\n",
    "optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# Durante el entrenamiento:\n",
    "model_output = (3*weights**2).sum()\n",
    "model_output.backward()\n",
    "optimizer.step()  # <------- Aqui realizamos el paso en descenso de gradiente automáticamente\n",
    "\n",
    "print(\"weights.grad, antes de reiniciar acumulacion de grad: \", weights.grad)\n",
    "\n",
    "optimizer.zero_grad() # <-------\n",
    "\n",
    "print('\\nweights: ', weights)\n",
    "print('\\nmodel_output: ', model_output)\n",
    "print(\"\\nweights.grad, después de reiniciar acumulacion de grad: \", weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística\n",
    "\n",
    "Tratamos con un problema de clasificación binaria. Estamos utilizando datos médicos de los datasets de la librería scikit-learn. Hay dos clases con diferentes atributos, algunas personas tienen cáncer y otras no. Queremos entrenar nuestro clasificador binario para poder hacer predicciones de probabilidad de cancer según atributos. \n",
    "\n",
    "Para casi cualquier modelo que entrenemos necesitamos hacer lo siguiente:\n",
    "\n",
    "0) Preparar datos  \n",
    "1) Definir modelo  \n",
    "2) Función de pérdida (loss) y optimizador  \n",
    "3) Ciclo de entrenamiento  \n",
    "4) Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'> \n",
      "\n",
      "Container object exposing keys as attributes.\n",
      "\n",
      "    Bunch objects are sometimes used as an output for functions and methods.\n",
      "    They extend dictionaries by enabling values to be accessed by key,\n",
      "    `bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.utils import Bunch\n",
      "    >>> b = Bunch(a=1, b=2)\n",
      "    >>> b['b']\n",
      "    2\n",
      "    >>> b.b\n",
      "    2\n",
      "    >>> b.a = 3\n",
      "    >>> b['a']\n",
      "    3\n",
      "    >>> b.c = 6\n",
      "    >>> b['c']\n",
      "    6\n",
      "    \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos del dataset de scikit-learn\n",
    "bc = datasets.load_breast_cancer()\n",
    "print(type(bc),'\\n')\n",
    "\n",
    "print(bc.__doc__)\n",
    "\n",
    "print(bc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de features es: 30\n"
     ]
    }
   ],
   "source": [
    "# Dividimos los datos\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f\"El número de features es: {n_features}\")\n",
    "\n",
    "# Dividimos los datos usando la funcion split de scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Scale\n",
    "# StandardScaler es una herramienta en scikit-learn que estandariza las características \n",
    "# eliminando la media y escalando a la varianza unitaria.\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convertimos los numpy.ndarray a tensores de pytorch\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# Cambiar dimensión\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de modelo\n",
    "\n",
    "Definimos un modelo lineal para clasificación binaria, es decir:\n",
    "\n",
    "1. Modelo Lineal:\n",
    "$$\n",
    "f = wx + b \n",
    "$$\n",
    "2.\tFunción de Activación (Sigmoide):\n",
    "$$\n",
    "z = \\sigma(f) \n",
    "$$\n",
    "\n",
    "El modelo lineal se define por una combinación lineal de las entradas y los pesos, seguido de una función de activación sigmoide para clasificación binaria. En el caso de clasificación multiclase en lugar de una sigmoide, se utiliza una función softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelo\n",
    "class Model(nn.Module): # Se hereda de la clase nn.Module\n",
    "    \n",
    "    # Constructor. Define el esqueleto del modelo. Hay que poner tamaños, etc...\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__() # Llama al constructor de la clase base para poder inicializar el módulo\n",
    "        \n",
    "        # Define una capa lineal. Es una capa full conected con n entradas y 1 salida   \n",
    "        self.linear = nn.Linear(n_input_features, 1) \n",
    "\n",
    "    # Método forward. Define como pasa la entrada a través del modelo para producir una salida\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pasa la salida de la capa lineal en una función sigmoide\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred # Devuelve la predicción\n",
    "\n",
    "# Crea una instancia del modelo\n",
    "model = Model(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (linear): Linear(in_features=30, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100 # Definimos el número de epocas\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss() # Función de pérdida. Binary Cross Entropy \n",
    "# Sthocastic Gradient Desc. Le pasamos los parametros del modelo\n",
    "print(model.parameters)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2395\n",
      "Epoch [20/100], Loss: 0.2300\n",
      "Epoch [30/100], Loss: 0.2216\n",
      "Epoch [40/100], Loss: 0.2141\n",
      "Epoch [50/100], Loss: 0.2073\n",
      "Epoch [60/100], Loss: 0.2011\n",
      "Epoch [70/100], Loss: 0.1954\n",
      "Epoch [80/100], Loss: 0.1902\n",
      "Epoch [90/100], Loss: 0.1855\n",
      "Epoch [100/100], Loss: 0.1810\n"
     ]
    }
   ],
   "source": [
    "# 3) Ciclo de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Forward pass. No hay necesidad de llamar al método forward\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(y_pred, y_train) # Encontramos el error entre la predicción y la realidad\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward() # Calculamos los gradientes\n",
    "\n",
    "    # Update\n",
    "    optimizer.step() # Actualizamos el paso del modelo\n",
    "\n",
    "    # Vaciamos el gradiente\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        # Solo imprimimos los multiplos de 10\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8095],\n",
      "        [0.4007],\n",
      "        [0.6788],\n",
      "        [0.5545],\n",
      "        [0.8653],\n",
      "        [0.6452],\n",
      "        [0.5068],\n",
      "        [0.5981],\n",
      "        [0.1797],\n",
      "        [0.7007]])\n"
     ]
    }
   ],
   "source": [
    "# Predicción ejemplo\n",
    "with torch.no_grad():\n",
    "    X_test = torch.randn(10, n_features)  # 10 ejemplos de prueba\n",
    "    y_test_pred = model(X_test)\n",
    "    print(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Para no acumular el gradiente\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    # Calculamos el accuracy\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un modelo de regresión logística, los pesos son parámetros que se ajustan durante el entrenamiento para minimizar la función de pérdida. En este ejemplo, los pesos están en la capa lineal del modelo.\n",
    "\n",
    "**Dónde están los Pesos**\n",
    "\n",
    "Los pesos en una red neuronal están contenidos en la capa lineal (nn.Linear). La clase nn.Linear tiene dos tipos de parámetros:\n",
    "* weight: La matriz de pesos que multiplica las entradas.\n",
    "* bias: El sesgo que se suma al resultado de la multiplicación de pesos.\n",
    "\n",
    "Cuando creas una instancia de nn.Linear, PyTorch inicializa estos pesos y sesgos automáticamente. Durante el entrenamiento, estos parámetros se ajustan para minimizar la función de pérdida.\n",
    "\n",
    "1.\tInicialización de Pesos:\n",
    "\t* Cuando creas la capa lineal self.linear = nn.Linear(n_input_features, 1), PyTorch inicializa automáticamente los pesos y sesgos de esta capa.\n",
    "2.\tPesos y Sesgos:\n",
    "\t* Puedes acceder a los pesos y sesgos directamente a través de los atributos de la capa lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2283, -0.0430, -0.1024, -0.1784, -0.2034, -0.0286, -0.1555, -0.3092,\n",
      "          0.0331, -0.0213, -0.0812, -0.0896, -0.2627, -0.1033,  0.0732,  0.1199,\n",
      "         -0.1208, -0.0759,  0.0262,  0.0709, -0.2755, -0.0570, -0.1559, -0.3434,\n",
      "         -0.0538, -0.2774, -0.1110, -0.3863, -0.1085, -0.1424]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2791], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.linear.weight)\n",
    "print(model.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tGradientes:\n",
    "\t* Durante la fase de backward pass (loss.backward()), PyTorch calcula los gradientes de la función de pérdida con respecto a cada parámetro (pesos y sesgos) del modelo. Estos gradientes se almacenan en los atributos grad de los parámetros.\n",
    "\t* Puedes acceder a los gradientes después de la backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.linear.weight.grad)\n",
    "print(model.linear.bias.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
