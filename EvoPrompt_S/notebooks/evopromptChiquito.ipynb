{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación en chiquito código EvoPrompt\n",
    "\n",
    "Guillermo Segura Gómez\n",
    "\n",
    "En este notebook se va segmentar el código de [EvoPrompt](https://github.com/beeevita/EvoPrompt) para su correcto funcionamiento. Además se tratará de utilizar prompts en español para ver su funcionamiento. \n",
    "\n",
    "El código de tratará de simplificar lo mas posible, por este motivo el subtitulo *chiquito*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    OPTForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    ")\n",
    "\n",
    "# Templates\n",
    "from data.templates import templates\n",
    "from data.template_ga import templates_2\n",
    "\n",
    "from datasets import Dataset as Dataset2\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "from utils import * # Utilidades\n",
    "from dataset import TextDataset # Libreria de configuración de los datos\n",
    "from llm_client import * # Libreria para llamar al modelo por API\n",
    "from metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evoluter:\n",
    "    def __init__(self, args, evaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.init_poplulation = []\n",
    "        self.population = []\n",
    "        self.scores = []\n",
    "        self.marks = []\n",
    "        self.client, self.llm_config = evaluator.client, evaluator.llm_config\n",
    "        self.public_out_path = self.evaluator.public_out_path\n",
    "\n",
    "        logger = self.logger = evaluator.logger\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"\\n\\t\" + \"\\n\\t\".join(f\"{k} = {v}\" for k, v in vars(args).items()))\n",
    "        logger.info(\"=\" * 50)\n",
    "        self.args = args\n",
    "\n",
    "        if args.task in [\"sim\", \"sum\"]:\n",
    "            self.eval_src, self.eval_tgt = evaluator.dev_src, evaluator.dev_tgt\n",
    "            self.eval_src = self.eval_src[: args.sample_num]\n",
    "            self.eval_tgt = [i[: args.sample_num] for i in self.eval_tgt]\n",
    "        elif args.task == \"qa\":\n",
    "            self.eval_src, self.eval_tgt = evaluator.dev_src, evaluator.dev_tgt\n",
    "        else:\n",
    "            self.eval_src, self.eval_tgt = load_cls_data(\n",
    "                evaluator.verbalizers, args.dev_file\n",
    "            )\n",
    "\n",
    "    def sorted(self):\n",
    "        best_score = 0\n",
    "        total_score = 0\n",
    "        with open(os.path.join(self.public_out_path, \"dev_result.txt\"), \"w\") as wf:\n",
    "            self.scores, self.population, self.marks = (\n",
    "                list(t)\n",
    "                for t in zip(\n",
    "                    *sorted(\n",
    "                        zip(self.scores, self.population, self.marks),\n",
    "                        key=lambda x: x[0],\n",
    "                        reverse=True,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for score, prompt, mark in zip(self.scores, self.population, self.marks):\n",
    "                score_str = \"\\t\".join([str(round(i, 4)) for i in score])\n",
    "                float_score = float(score[-1])\n",
    "                if float_score > best_score:\n",
    "                    best_score = float_score\n",
    "                total_score += float_score\n",
    "                wf.write(f\"{mark}\\t{prompt}\\t{score_str}\\n\")\n",
    "            wf.write(f\"best score: {best_score}\\n\")\n",
    "            wf.write(f\"average score: {total_score / len(self.scores)}\\n\")\n",
    "            wf.close()\n",
    "\n",
    "    def init_pop(self):\n",
    "        args = self.args\n",
    "        evaluator = self.evaluator\n",
    "        dataset = args.dataset\n",
    "        prompts2mark = {}\n",
    "        manual_prompt_path = f\"./data/{args.task}/{dataset}/prompts.txt\"\n",
    "        ape_prompt_path = f\"./data/{args.task}/{dataset}/prompts_auto.txt\"\n",
    "        if \"gpt\" in args.language_model or \"opt\" in args.language_model:\n",
    "            model = f\"_{args.language_model}\"\n",
    "        else:\n",
    "            model = \"\"\n",
    "\n",
    "        manual_pop = read_lines(manual_prompt_path)\n",
    "        try:\n",
    "            ape_pop = read_lines(ape_prompt_path)\n",
    "        except:\n",
    "            ape_pop = []\n",
    "        for p in ape_pop:\n",
    "            prompts2mark[p] = \"ape\"\n",
    "        for p in manual_pop:\n",
    "            prompts2mark[p] = \"manual\"\n",
    "\n",
    "        self.evaluated_prompts = {}\n",
    "        logger = self.logger\n",
    "        out_path = self.public_out_path\n",
    "        cur_budget = -1\n",
    "        if args.initial == \"all\":\n",
    "            cache_path = (\n",
    "                args.cache_path\n",
    "                if args.cache_path\n",
    "                else f\"./data/{args.task}/{dataset}/seed{args.seed}/prompts{model}.json\"\n",
    "            )\n",
    "            try:\n",
    "                self.evaluated_prompts = json.load(open(cache_path, \"r\"))\n",
    "                logger.info(f\"---loading prompts from {cache_path}\")\n",
    "                metric_index = -1\n",
    "                self.evaluated_prompts = dict(\n",
    "                    sorted(\n",
    "                        self.evaluated_prompts.items(),\n",
    "                        key=lambda item: item[1][metric_index],\n",
    "                        reverse=True,\n",
    "                    )\n",
    "                )\n",
    "                init_population = [k for k in list(self.evaluated_prompts.keys())]\n",
    "            except:\n",
    "                topk_population = []\n",
    "                logger.info(\n",
    "                    \"-----evaluating initial population and paraphrasing topk---------\"\n",
    "                )\n",
    "                for prompt in manual_pop + ape_pop:\n",
    "                    eval_res = evaluator.forward(prompt, self.eval_src, self.eval_tgt)\n",
    "                    scores = eval_res[\"scores\"]\n",
    "                    self.evaluated_prompts[prompt] = scores\n",
    "                    topk_population.append((scores[-1], prompt))\n",
    "                topk_population.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "                with open(cache_path, \"w\") as wf:\n",
    "                    self.evaluated_prompts = dict(\n",
    "                        sorted(\n",
    "                            self.evaluated_prompts.items(), key=lambda item: item[1][0]\n",
    "                        )\n",
    "                    )\n",
    "                    json.dump(self.evaluated_prompts, wf)\n",
    "                init_population = [i[1] for i in topk_population]\n",
    "        elif args.initial == \"ape\":\n",
    "            init_population = read_lines(ape_prompt_path)[: args.popsize]\n",
    "            prompts2mark = {i: \"ape\" for i in init_population}\n",
    "        elif args.initial == \"ckpt\":\n",
    "            init_population = []\n",
    "            logger.info(f\"------------load from file {args.ckpt_pop}------------\")\n",
    "            ckpt_pop = read_lines(args.ckpt_pop)[: args.popsize]\n",
    "            for line in ckpt_pop:\n",
    "                try:\n",
    "                    elements = line.split(\"\\t\")\n",
    "                    mark, prompt = elements[0], elements[1]\n",
    "                    score = elements[2:]\n",
    "                    score = [float(i) for i in score]\n",
    "                except:\n",
    "                    continue\n",
    "                prompts2mark[prompt] = mark\n",
    "                self.evaluated_prompts[prompt] = [i for i in score]\n",
    "                init_population.append(prompt)\n",
    "            # print(init_population)\n",
    "            cur_budget = extract_numbers(args.ckpt_pop.split(\"/\")[-1])\n",
    "            logger.info(\"cur budget is {}\".format(cur_budget))\n",
    "\n",
    "        client = evaluator.client\n",
    "        llm_config = evaluator.llm_config\n",
    "\n",
    "        # test LLM\n",
    "        _ = paraphrase(\n",
    "            sentence=\"Hi, I am a student.\",\n",
    "            type=args.llm_type,\n",
    "            client=client,\n",
    "            temperature=0.5,\n",
    "            **llm_config,\n",
    "        )\n",
    "        logger.info(\"test LLM client success\")\n",
    "        if args.initial_mode in [\"para_topk\", \"para_bottomk\", \"para_randomk\"]:\n",
    "            k_pop = k_init_pop(args.initial_mode, init_population, k=args.popsize)\n",
    "            logger.info(\"-----paraphrasing topk---------\")\n",
    "            para_population = paraphrase(\n",
    "                client=client, sentence=k_pop, type=args.llm_type, **llm_config\n",
    "            )\n",
    "            for p in para_population:\n",
    "                prompts2mark[p] = \"para\"\n",
    "                score = evaluator.forward(p, self.eval_src, self.eval_tgt)[\"scores\"]\n",
    "                self.evaluated_prompts[p] = score\n",
    "            init_population = k_pop + para_population\n",
    "            print(init_population)\n",
    "            init_population = init_population[: args.popsize]\n",
    "        elif args.initial_mode in [\"topk\", \"bottomk\", \"randomk\"]:\n",
    "            init_population = k_init_pop(\n",
    "                args.initial_mode, init_population, k=args.popsize\n",
    "            )\n",
    "\n",
    "        self.population = [i for i in init_population]\n",
    "        assert len(self.population) == args.popsize\n",
    "\n",
    "        for i in self.population:\n",
    "            logger.info(i)\n",
    "        with open(f\"{out_path}/step0_pop_para.txt\", \"w\") as wf:\n",
    "            for prompt in self.population:\n",
    "                score_str = \"\\t\".join(\n",
    "                    [str(round(i, 4)) for i in self.evaluated_prompts[prompt]]\n",
    "                )\n",
    "                wf.write(f\"{prompts2mark[prompt]}\\t{prompt}\\t{score_str}\\n\")\n",
    "\n",
    "        self.prompts2mark = prompts2mark\n",
    "        return self.evaluated_prompts, cur_budget\n",
    "\n",
    "    def write_step(self, step, best_score, avg_score):\n",
    "        with open(os.path.join(self.public_out_path, f\"step{step}_pop.txt\"), \"w\") as wf:\n",
    "            for p in self.population:\n",
    "                score_str = \"\\t\".join(\n",
    "                    [str(round(i, 4)) for i in self.evaluated_prompts[p]]\n",
    "                )\n",
    "                wf.write(self.prompts2mark[p] + \"\\t\" + p + \"\\t\" + score_str + \"\\n\")\n",
    "            wf.write(f\"best score: {best_score}\\n\")\n",
    "            wf.write(f\"average score: {avg_score}\\n\")\n",
    "\n",
    "    def evolute(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAEvoluter(Evoluter):\n",
    "    def __init__(self, args, evaluator):\n",
    "        super(GAEvoluter, self).__init__(args, evaluator)\n",
    "        try:\n",
    "            self.template = templates_2[args.task]\n",
    "        except:\n",
    "            self.template = templates_2[\"sim\"]\n",
    "\n",
    "    def evolute(self):\n",
    "        logger = self.logger\n",
    "        self.evaluated_prompts, cur_budget = self.init_pop()\n",
    "        evaluator = self.evaluator\n",
    "        args = self.args\n",
    "        eval_src = self.eval_src\n",
    "        eval_tgt = self.eval_tgt\n",
    "        out_path = self.public_out_path\n",
    "        template = self.template\n",
    "\n",
    "        best_scores = []\n",
    "        avg_scores = []\n",
    "\n",
    "        cur_best_prompt, cur_best_score = max(\n",
    "            self.evaluated_prompts.items(), key=lambda x: x[1][0]\n",
    "        )\n",
    "        cur_best_score = cur_best_score[-1]\n",
    "        fitness = np.array([self.evaluated_prompts[i][0] for i in self.population])\n",
    "\n",
    "        for step in range(cur_budget + 1, args.budget):\n",
    "            total_score = 0\n",
    "            best_score = 0\n",
    "            fitness = np.array([self.evaluated_prompts[i][0] for i in self.population])\n",
    "            new_pop = []\n",
    "            if args.sel_mode == \"wheel\":\n",
    "                wheel_idx = np.random.choice(\n",
    "                    np.arange(args.popsize),\n",
    "                    size=args.popsize,\n",
    "                    replace=True,\n",
    "                    p=fitness / fitness.sum(),\n",
    "                ).tolist()  # a temp pop to select parents\n",
    "                parent_pop = [self.population[i] for i in wheel_idx]\n",
    "            elif args.sel_mode in [\"random\", \"tour\"]:\n",
    "                parent_pop = [i for i in self.population]\n",
    "\n",
    "            for j in range(args.popsize):\n",
    "                logger.info(f\"step {step}, pop {j}\")\n",
    "                # print(np.random.choice(np.arange(args.popsize), size=2, replace=True,\n",
    "                # p=fitness/fitness.sum()).tolist())\n",
    "                if args.sel_mode in [\"random\", \"wheel\"]:\n",
    "                    parents = random.sample(parent_pop, 2)\n",
    "                    cand_a = parents[0]\n",
    "                    cand_b = parents[1]\n",
    "                elif args.sel_mode == \"tour\":\n",
    "                    group_a = random.sample(parent_pop, 2)\n",
    "                    group_b = random.sample(parent_pop, 2)\n",
    "                    cand_a = max(group_a, key=lambda x: self.evaluated_prompts[x][0])\n",
    "                    cand_b = max(group_b, key=lambda x: self.evaluated_prompts[x][0])\n",
    "\n",
    "                request_content = template.replace(\"<prompt1>\", cand_a).replace(\n",
    "                    \"<prompt2>\", cand_b\n",
    "                )\n",
    "                # logger.info(f\"old_child: {old_prompt}, {old_score}\")\n",
    "                logger.info(\"evolution example:\")\n",
    "                logger.info(request_content)\n",
    "                logger.info(\"parents:\")\n",
    "                logger.info(cand_a)\n",
    "                logger.info(cand_b)\n",
    "                child_prompt = llm_query(\n",
    "                    client=self.client,\n",
    "                    data=request_content,\n",
    "                    type=args.llm_type,\n",
    "                    task=False,\n",
    "                    temperature=0.5,\n",
    "                    **self.llm_config,\n",
    "                )\n",
    "                logger.info(f\"original child prompt: {child_prompt}\")\n",
    "                child_prompt = get_final_prompt(child_prompt)\n",
    "                logger.info(f\"child prompt: {child_prompt}\")\n",
    "\n",
    "                de_eval_res = evaluator.forward(child_prompt, eval_src, eval_tgt)\n",
    "                de_hypos = de_eval_res[\"hypos\"]\n",
    "                de_scores = de_eval_res[\"scores\"]\n",
    "                de_score_str = \"\\t\".join([str(round(i, 4)) for i in de_scores])\n",
    "                new_score = de_scores[-1]\n",
    "\n",
    "                logger.info(f\"new score: {de_score_str}\")\n",
    "                self.prompts2mark[child_prompt] = \"evoluted\"\n",
    "\n",
    "                self.evaluated_prompts[child_prompt] = de_scores\n",
    "                if args.ga_mode == \"std\":\n",
    "                    selected_prompt = child_prompt\n",
    "                    selected_score = new_score\n",
    "                    self.population[j] = selected_prompt\n",
    "\n",
    "                elif args.ga_mode == \"topk\":\n",
    "                    selected_prompt = child_prompt\n",
    "                    selected_score = new_score\n",
    "\n",
    "                new_pop.append(selected_prompt)\n",
    "                total_score += selected_score\n",
    "                if selected_score > best_score:\n",
    "                    best_score = selected_score\n",
    "                    if best_score > cur_best_score:\n",
    "                        cur_best_score = best_score\n",
    "\n",
    "            # self.population = new_pop\n",
    "            if args.ga_mode == \"topk\":\n",
    "                double_pop = list(set(self.population + new_pop))\n",
    "                double_pop = sorted(\n",
    "                    double_pop,\n",
    "                    key=lambda x: self.evaluated_prompts[x][-1],\n",
    "                    reverse=True,\n",
    "                )\n",
    "                self.population = double_pop[: args.popsize]\n",
    "                total_score = sum(\n",
    "                    [self.evaluated_prompts[i][-1] for i in self.population]\n",
    "                )\n",
    "                best_score = self.evaluated_prompts[self.population[0]][-1]\n",
    "            avg_score = total_score / args.popsize\n",
    "            avg_scores.append(avg_score)\n",
    "            best_scores.append(best_score)\n",
    "\n",
    "            self.write_step(step, best_score, avg_score)\n",
    "\n",
    "            if step == args.budget - 1:\n",
    "                logger.info(f\"----------testing step {step} self.population----------\")\n",
    "                pop_marks = [self.prompts2mark[i] for i in self.population]\n",
    "                pop_scores = [self.evaluated_prompts[i] for i in self.population]\n",
    "                self.population, pop_scores, pop_marks = (\n",
    "                    list(t)\n",
    "                    for t in zip(\n",
    "                        *sorted(\n",
    "                            zip(self.population, pop_scores, pop_marks),\n",
    "                            key=lambda x: x[1][-1],\n",
    "                            reverse=True,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                test_prompt_num = 3\n",
    "                best_score, best_prompt = evaluate_optimized_prompt(\n",
    "                    self.population[:test_prompt_num],\n",
    "                    pop_marks[:test_prompt_num],\n",
    "                    os.path.join(out_path, f\"step{step}_pop_test.txt\"),\n",
    "                    evaluator,\n",
    "                    args,\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"----------step {step} best score: {best_score}, best prompt: {best_prompt}----------\"\n",
    "                )\n",
    "\n",
    "        best_scores = [str(i) for i in best_scores]\n",
    "        avg_scores = [str(round(i, 4)) for i in avg_scores]\n",
    "        logger.info(f\"best_scores: {','.join(best_scores)}\")\n",
    "        logger.info(f\"avg_scores: {','.join(avg_scores)}\")\n",
    "        self.scores = [self.evaluated_prompts[i] for i in self.population]\n",
    "        self.marks = [self.prompts2mark[i] for i in self.population]\n",
    "        self.sorted()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
